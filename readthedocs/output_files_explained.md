# Workflow Output Files Documentation

This document describes all files and directories generated by the JGI Integration Workflow.

## Directory Structure Overview

The workflow creates a hierarchical output structure with hash-tagged directories to track different processing and analysis configurations:

```
output_data/
└── <project_name>/
    ├── Dataset_Processing--<hash>/
    │   ├── Analysis--<hash>/
    │   ├── metabolomics/
    │   ├── transcriptomics/
    ├── configs/
    └── notebooks/
```

- **Dataset_Processing--<hash>**: Contains all outputs for a specific data processing configuration (first step that filters, devariances, normalizes, and transforms/scales the data). The hash uniquely identifies all the parameters used in the configuration file, and multiple different analyses can be run on the same processed data.
- **Analysis--<hash>**: Contains all outputs for a specific analysis configuration. The hash uniquely identifies the feature selection, correlation, and network parameters used. Each analysis is unique, depending on how you set up the configuration file.

---

## Top-Level Directories

### `configs/`
Contains persistent configuration files with hash-tagged filenames that allow you to reproduce specific analyses.

| File | Description |
|------|-------------|
| `Dataset_Processing--<hash>_Analysis--<hash>_project_config.yml` | Project-level settings including paths, dataset names, and global parameters |
| `Dataset_Processing--<hash>_Analysis--<hash>_data_processing_config.yml` | Data processing parameters including filtering thresholds, scaling methods, and variance cutoffs |
| `Dataset_Processing--<hash>_Analysis--<hash>_analysis_config.yml` | Analysis parameters including correlation thresholds, network settings, and feature selection criteria |

### `notebooks/`
Contains Jupyter notebooks with embedded execution history.

| File | Description |
|------|-------------|
| `Dataset_Processing--<hash>_Analysis--<hash>_notebook.ipynb` | Complete analysis notebook with all code cells, outputs, and visualizations. Hash-tagged for reproducibility. |

---

## Dataset-Specific Outputs

Both `metabolomics/` and `transcriptomics/` directories contain the same file structure representing the data processing pipeline stages.

### Core Data Files

| File | Description |
|------|-------------|
| `raw_data.csv` | Original unprocessed feature abundance data (features × samples) |
| `raw_metadata.csv` | Original sample metadata with experimental conditions and groupings |
| `linked_data.csv` | Feature data after linking samples between datasets (only samples present in both omics types) |
| `linked_metadata.csv` | Metadata after linking samples between datasets |
| `filtered_data.csv` | Data after removing rare features based on presence/absence thresholds (optional step) |
| `devarianced_data.csv` | Data after removing low-variance features (optional step) |
| `scaled_data.csv` | Data after log2 transformation and scaling |
| `normalized_data.csv` | Fully processed data after optional replicability filtering |
| `annotation_table.csv` | Feature annotations including functional classifications, pathways, and identifiers |

### PCA Plots (`pca_plots/`)
Principal Component Analysis visualizations at different processing stages.

| File | Description |
|------|-------------|
| `pca_grid.pdf` | Multi-panel grid showing PCA plots for all metadata variables and processing stages (useful for checking pre- and post-processing data structure) |
| `pca_of_<dataset>_linked_by_<variable>.pdf` | PCA of linked data colored by specific metadata variable (e.g., timepoint, treatment group) |
| `pca_of_<dataset>_normalized_by_<variable>.pdf` | PCA of fully normalized data colored by specific metadata variable |

---

## Integrated Analysis Outputs

Located in `Analysis--<hash>/` directory.

### Core Integration Files

| File | Description |
|------|-------------|
| `integrated_metadata.csv` | Combined metadata for samples present across all datasets |
| `integrated_data.csv` | Combined feature data from all omics types (transcriptomics + metabolomics features × samples) |
| `feature_annotation_table.csv` | Comprehensive annotation table for all integrated features with dataset type identifiers |
| `integrated_data_selected.csv` | Subset of integrated data after statistical feature selection (e.g., Kruskall-Wallis test filtering) |

### Correlation and Network Files

| File | Description |
|------|-------------|
| `feature_correlation_table.csv` | Pairwise correlation coefficients and p-values for all feature pairs passing significance thresholds |
| `feature_network_graph.graphml` | Network graph in GraphML format where nodes are features and edges are significant correlations. Can be opened in Cytoscape or other network visualization tools. |
| `feature_network_edge_table.csv` | Table of network edges with correlation coefficients, p-values, and connected feature IDs |
| `feature_network_node_table.csv` | Table of network nodes with feature annotations, submodule assignments, and network statistics (degree, betweenness, etc.) |

### Functional Enrichment

| File | Description |
|------|-------------|
| `functional_enrichment_table.csv` | Statistical enrichment of functional categories within network submodules (compared to the whole network) using Fisher's exact test. Includes fold enrichment, p-values, and FDR corrections. |

### Database Files

| File | Description |
|------|-------------|
| `analysis_data.db` | DuckDB database containing all analysis tables for SQL queries and AI-powered exploration |
| `analysis_data.db.wal` | Write-ahead log for DuckDB (temporary file for database transactions) |

### Distribution Plots

| File | Description |
|------|-------------|
| `<Dataset>_Peak_Heights.pdf` or `<Dataset>_Counts.pdf` | Histogram showing distribution of feature abundances for each dataset type |
| `distribution_of_normalized_datasets.pdf` | Combined histogram comparing abundance distributions across all datasets after normalization |
| `PCA_Plot___<variable>.pdf` | PCA visualization of integrated data colored by specific metadata variable |

---

## Submodule Analysis Outputs

Located in `Analysis--<hash>/submodules/` directory.

Network submodules are densely connected clusters of co-varying features identified using community detection algorithms.

### Per-Submodule Files

Each submodule generates three files:

| File Pattern | Description |
|--------------|-------------|
| `feature_network_graph_submodule<N>.graphml` | GraphML network file for submodule N containing only features within that community |
| `feature_network_graph_submodule<N>_edges.csv` | Edge table for submodule N with correlation values between features |
| `feature_network_graph_submodule<N>_nodes.csv` | Node table for submodule N with feature annotations and within-submodule network statistics |

**Note**: Submodule numbering starts at 1 and continues based on community detection results.

---

## Network Analyzer Results

Located in `Analysis--<hash>/network_analyzer_results/` directory.

These files are generated by the `run_full_network_analyzer()` function and provide comparative network analysis across different feature types. This folder contains results that allow you to measure information gained about your dataset after integration vs if you had analyzed the datasets independently.

### Network Comparison Files

| File Pattern | Description |
|--------------|-------------|
| `network_comparison_full_network.graphml` | Complete correlation network including all feature types |
| `network_comparison_full_edges.csv` | All edges in the complete network with correlation statistics |
| `network_comparison_full_nodes.csv` | All nodes in the complete network with topological properties |
| `network_comparison_<dataset>_only_network.graphml` | Subnetwork containing only features from specific dataset (e.g., mx_only, tx_only) |
| `network_comparison_<dataset>_only_edges.csv` | Edges within dataset-specific subnetwork |
| `network_comparison_<dataset>_only_nodes.csv` | Nodes within dataset-specific subnetwork with recalculated network statistics |

**Use case**: Compare network topology and feature connectivity patterns between and within omics types.

---

## Custom Visualization Outputs

Located in `Analysis--<hash>/boxplots/` directory.

### Individual Feature Plots

| File Pattern | Description |
|--------------|-------------|
| `abundance_of_<feature_id>_by_<metadata_variable>.pdf` | Boxplot showing abundance distribution of a specific feature across metadata categories (e.g., treatment groups, timepoints) |

### Submodule Abundance Plots

| File Pattern | Description |
|--------------|-------------|
| `avg_abundance_of_submodule_<N>_nodes_by_<metadata_variable>.pdf` | Boxplot showing average abundance of all features within submodule N across metadata categories. Useful for identifying submodules that respond to specific conditions. |

**Note**: These files are generated on-demand using the `plot_individual_feature()` and `plot_submodule_avg_abundance()` methods.

---

## File Format Descriptions

### CSV Files
- **Indexing**: Most CSV files use the first column as the index (sample IDs for metadata, feature IDs for data matrices)
- **Delimiter**: Comma-separated
- **Encoding**: UTF-8
- **Data matrices**: Features as rows, samples as columns (standard convention)

### GraphML Files
- **Format**: XML-based graph markup language
- **Compatible software**: Cytoscape, Gephi, NetworkX (Python), igraph (R)
- **Attributes**: Nodes contain feature annotations; edges contain correlation coefficients and p-values

### PDF Files
- **Resolution**: 300 DPI (publication quality)
- **Size**: Varies by plot type (typically 8-16 inches width)
- **Fonts**: Embedded for portability

---

## Reproducing Analyses

The hash-based file naming system enables exact reproduction of results:

1. **Identify your hashes**: Look at the directory names (e.g., `Dataset_Processing--94368992` and `Analysis--c5064fb0`)
2. **Load configuration**: Use the hash-tagged config files in the `configs/` directory
3. **Initialize with hashes**: In your notebook, initialize the workflow with:
   ```python
   project = Project(data_processing_hash='94368992', analysis_hash='c5064fb0')
   ```
4. **Run analysis**: Execute the workflow and you'll get identical results (deterministic processing)

---

## Data Provenance

All output files contain provenance information linking them to specific parameter configurations:

- **Configuration hashes**: Uniquely identify parameter sets used
- **Persistent configs**: Stored in `configs/` directory for future reference
- **Notebook snapshots**: Complete execution history saved in `notebooks/` directory
- **File naming**: Incorporates hashes for unambiguous tracking

This system ensures scientific reproducibility and allows you to compare results across different parameter choices.

---

## Common Analysis Workflows

### 1. Exploring Network Submodules
- Start with: `feature_network_node_table.csv`
- Identify submodules of interest based on size and functional enrichment
- Examine: `submodules/feature_network_graph_submodule<N>_nodes.csv`
- Visualize: Load `submodules/feature_network_graph_submodule<N>.graphml` in Cytoscape

### 2. Feature Investigation
- Start with: `integrated_data_selected.csv` (statistically significant features)
- Check annotations: `feature_annotation_table.csv`
- Plot individual features: Use `analysis.plot_individual_feature()` method
- Check correlations: `feature_correlation_table.csv`

### 3. Functional Enrichment Analysis
- Start with: `functional_enrichment_table.csv`
- Filter by corrected p-value (typically < 0.05)
- Sort by fold enrichment to find strongest associations
- Cross-reference with: `feature_network_node_table.csv` to see which features belong to enriched categories

### 4. Cross-Omics Integration
- Compare: `network_analyzer_results/network_comparison_<dataset>_only_nodes.csv` files
- Look for trans-omics edges connecting metabolites and genes in `feature_network_edge_table.csv`
- Use the DuckDB database (`analysis_data.db`) for complex queries across multiple tables

---

## Database Query Examples

The `analysis_data.db` file supports SQL queries for flexible data exploration. See the exploration.md documentation for detailed analysis about running natural language queries against your data.

---

## Tips for Working with Output Files

1. **Large networks**: For networks with >1000 nodes, consider filtering by correlation threshold before visualization
2. **Feature selection**: The `integrated_data_selected.csv` file contains pre-filtered features and is recommended for most downstream analyses
3. **Submodule analysis**: Focus on submodules with >5 nodes for robust functional enrichment
4. **Cross-omics connections**: Filter `feature_network_edge_table.csv` for edges connecting features from different datasets (e.g., tx-mx edges)
5. **PCA interpretation**: Check the PCA grid first to identify which metadata variables show the strongest separation
6. **Reproducibility**: Always record your dataset processing and analysis hashes for future reference

---

## Troubleshooting

**Missing files?**
- Check that the corresponding workflow step completed successfully
- Verify `overwrite=False` wasn't preventing regeneration
- Look for error messages in the notebook output

**Empty files?**
- May indicate all features were filtered out due to strict thresholds
- Check filtering parameters in your config files
- Review intermediate files to identify where data was lost

**Corrupted database files?**
- Delete `.db.wal` file and reload
- Regenerate using `analysis.register_all_existing_data()`

---

## Additional Resources

- **Setup documentation**: See `setup.md` for installation and environment configuration
- **Configuration parameters**: See `*_config_parameters_explained.md` files for detailed parameter descriptions
- **Running the workflow**: See `run.md` for step-by-step execution guide
- **Data exploration**: See `exploration.md` for advanced query and visualization techniques
